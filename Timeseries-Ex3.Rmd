---
title: "TMA4285 Timeseries, Exercise 3"
author: "Marius Dioli, Amir Ahmed and Andreas Ferstad"
date: "September 2019"
output:
  html_document: default
  word_document: default
  pdf_document:
      fig_caption: yes
bibliography: bibliography.bib
nocite: '@*'
  
header-includes:
 \usepackage{float}
 \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	error = FALSE,
	message = FALSE,
	warning = FALSE,
	include = F,
	echo = F,
	fig.pos="H",
	fig.align='center',
	fig.height = 4.75,
	fig.width = 10
)
knitr::opts_chunk$set(error=TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")

```


```{r include=FALSE}
#Imports
library("knitr") #probably already installed
library("rmarkdown") #probably already installed
library(astsa)
library(forecast)
library(dplyr)
library(ggplot2)
library(DescTools)
```

# Abstract
 This article presents the basics of time series analysis, and applies it to model monthly average atmospheric CO2 levels observed at Mauna Loa Observatory, Hawaii, U.S. 

# Introduction
Carbon dioxide levels have been monitored since 1958 at the Mauna Loa Observatory, located in the Pacific Ocean on top of Hawaiiâ€™s biggest volcano. CO2 is a green-house gas, and as humans burn fossile fuels, more is added to the atmosphere. As a consequence we have since the industrial revolution seen an increase in global average temperature. In the Paris agreement one targeted to stay below a 2C increase, commonly that has been seen to be equivalent to about 450ppm in atmospheric CO2. In this article we will use theory on time series to model CO2 levels measured at the observatory, with the goal of forecasting future levels, and to predict when, if development continues as today, when will reach the 450ppm roof. For analysis, we will focus on flask measurements taken on a monthly basis as we are interested in predicting the long-term trend. 

# Theory
We note that large parts of the theory presented here, is based on Brockwell and Davis. 

A timeseries can be considered as a stochastic process, a set of random variables indexed in accordance to their occurrence relative to the other measurements. We will organize the finite timeseries we will deal with into vectors: $\mathbf{X}= ( X_1, X_2, \dots,X_n)^T$. A realization of a timeseries is an observation of the the real values of the time series, and will be denoted as $\mathbf{x} = (x_1, x_2, \dots, x_n)^T$

The covariance function of two elements of  $\mathbf{X}$ is defined as:
$$\gamma_X(r,s)=Cov(X_r, X_s)=E((X_r-E(X_r))(X_s-E(X_s))$$
In general the covariance function tells us how observations correlate to each other. 

A time series is weakly stationary if:

* $E(X_t)$ is independent of $t$

* $\gamma_X(t+h, t)$ is independent of t at each h value. 

In a weakly stationary timesries the correlation between two observations would only be depended the relative time between the observation of the repsective observations. 

The auto correlation function at a selected lag $h$ is: $$\rho_X(h)=\dfrac{\gamma_X(h)}{\gamma_X(0)}$$
Where lagging a time-series is shifting it forward by a selected amount of indexes. 

The estimator of the autocovariance of a realized time-series, referred to as the sample co-variance is:
$$\hat{\gamma}(h)=\frac{1}{n}\sum_{t=1}^{n-|h|}(x_{t+|h|}-\bar{x})(x_{t}-\bar{x})$$
For white noise time-series sample autocorrolation $\gamma(h)\sim N(0,\frac{1}{n})$ distribution. Under general assumptions this estimator is for large samples close to unbiased.  

The time series we will observe in the data-analysis later, is classically decomposed in the following manner:
$$X_t = m_t + s_t + Y_t, \quad t=1,\dots,n$$
Where $EY_t = 0$, $s_{t+d}=s_t$ and $\sum_{j=1}^d s_j= 0$.$m$ is referred to as the  trend of the time-series, and $s_t$ is the seasonal trend. The noise $Y_t$ is often assumed to be identically independently distributed (iid) white noise:

$$Y_t \sim WN (0,\sigma^2)$$
Subtracting the trend estimation, will yield as timeseries without a trend. 
A trend can also be eliminated from a series by differencing:
$$\nabla X_t =X_t-X_{t-1}=(1-B)X_t$$
This is referred to as lag-1 difference, where B is the backward shift operator. 
$$BX_t = X_{t-1}$$
Can difference n-times by:
$$\nabla^n X_t = (1-B)^nX_t$$
Some data, as stated earlier, show a seasonal trend, this can e.g. be that months of different years correlate, applying a lag-d difference operator is a way of reducing such a case to a stationary process, a lag-d operator can be defined as:
$$\nabla_dX_t = X_t-X_{t-d} = (1-B^d)X_t$$

A time series $\mathbf{X}$ is a linear process if:
$$X_t = \sum_{j=-\infty}^{\infty}\phi _jY_j$$
with $\sum_{j=-\infty}^{\infty}|\phi _j|<\infty$. Where $\mathbf{Y}$ is white noise. 

## ARMA and ARIMA
This brings us to the ARMA and SARIMA models, the latter which will be a central part in our analysis.  An $ARMA(p,q)$ model is defined as the following:
$$\phi(B)X_t = \theta(B)Z_t $$
where $Z_t$ is white noise, iid with variance $\sigma^2$

and the SARIMA;
$$\phi(B)\Phi(B)X_t = \theta(B)\Theta(B)Z_ t$$
More specifically we have:
$$ARMA(p,q):\quad Y_t=(1-B)^dX_t$$
$$SARIMA(p,d,q)\times(P,D,Q)_s:\quad Y_t=(1-B)^d(1-B^s)^DX_t$$
Where:
$$\theta(z)=1+\theta_1z+\dots+\theta_ qz^q$$
$$\Theta(z)=1+\Theta_1z+\dots+\Theta_ Qz^Q$$
$$\phi(z)=1-\phi_1z+\dots+\phi_ pz^p$$
$$\Phi(z)=1-\Phi_1z+\dots+\Phi_Pz^P$$
Multiplying the polynomials together, this is equivalent to a $ARMA(p+sP,q+sQ)$ model, and fitting it would thus in essence be equivalent to fitting an ARMA model.  

## The innovations algorithm
To fit the model one can use the innovations algorithim. Suppose $\{ X_t\}$ is zero mean, with $E(X_t)^2<\infty$ for all t and suppose $$E(X_iX_j)=\kappa(i,j)$$
Denote the best linear predictors, and their mean squared erros as:
$$\hat X_n =\left\{
                \begin{array}{ll}
                  0, n=1 \\
                  P_{n-1}X_n, n \geq 2
                \end{array}
              \right.$$
$$\upsilon_n = E(X_{n+1}-P_nX_{n+1})^2$$
Where $P_i$ is the projection onto the space spanned by the $i$ first elements of the timeseries in the $L^2(\Omega)$ hilbert-space with inner-product: $$<X_i, X_j> = E(X_iX_j)$$

Define the innovations as:

$$U_n=X_n-\hat X_n$$ 

and let $\mathbf{U}$ be a vector with the innovations ranging from 1 to n as its elements. As all $X_n$ are assumed to be the result of some linear combination of the preceding values there exists a matrix A such that

$$\mathbf{U}_n=A_n\mathbf{X}_n$$

$$A = \left[\begin{array}{lllll}
1            & 0            & 0            & \dots & 0 \\
a_{11}       & 1            & 0            & \dots & 0 \\
\vdots       & \vdots       & \vdots       & \ddots & \vdots \\
a_{n-1,-n-1} & a_{n-1,-n-2} & a_{n-1,-n-3} & \dots & 1
\end{array}
\right]$$

This matrix is invertible, as it is is unit lower triangular (it has a non-zero determinant of 1). 
The inverse will be on the following form:
$$C_n = \left[
\begin{array}{lllll}
1            & 0            & 0            & \dots & 0 \\
\theta_{11}       & 1            & 0            & \dots & 0 \\
\vdots       & \vdots       & \vdots       & \ddots & \vdots \\
\theta_{n-1,-n-1} & \theta_{n-1,-n-2} & \theta_{n-1,-n-3} & \dots & 1
\end{array}
\right]$$
$$A_nC_n=C_nA_n=I_n$$
This yields:

$$\hat{\mathbf{X}}_n = \mathbf{X}_n-\mathbf{U}_n = C_n\mathbf{U}_n - \mathbf{U}_n=C_n\mathbf{X}_n-C_n\hat{\mathbf{X}}_n-\mathbf{X}_n+\hat{\mathbf{X}}_n=(C_n-I_n)(\mathbf{X}_n-\hat{\mathbf{X}}_n)$$
Define $$\Theta_n = C_n - I_n$$
$$\Rightarrow \hat{\mathbf{X}}_n=\Theta_n(\mathbf{X}_n-\hat{\mathbf{X}_n})$$
Combining this with the fact that:
$$\mathbf{X}_n = C_n(\mathbf{X}_n-\hat{\mathbf{X}_n})$$
solving for the coefficients above can easily done numerically, Brockwell and Davis suggest calculating recursively yielding the innovation algorithm:
$$\upsilon_0=\kappa(1,1)$$
$$\theta_{n,n-k}=\upsilon_k^{-1}(\kappa(n+1, k+1)-\sum_{j=0}^{k-1}\theta_{k,k-j}\theta$$
$$\upsilon_n = \kappa(n+1, n+1)-\sum_{j=0}^{n-1}\theta_{ n,n-j} ^2\upsilon_j$$
## ARIMA and the innovation algorithm
Forecasting ARMA process using the innovation algorithim. Assume we have an ARMA process with known (p,q) values. We transform the process into:
$$           \left\{
                \begin{array}{ll}
                  W_t = \sigma^{-1}X_t, \quad t=1,\dots,m\\
                  W_t = \sigma^{-1}\phi(B)X_t, \quad t>m
                \end{array}
              \right.
$$
$m=max(p,q)$
The autocovariance can then be found using:
$$\kappa(i,j)=\left\{
                \begin{array}{ll}
                  \sigma^{-2}\gamma_X(i-j), \quad 1\leq i,j \leq m \\
                  \sigma^{-2}(\gamma_x(i-j)-\sum_{i=1}^p\phi_r\gamma_X(r-|i-j|)), \quad min(i,j)\leq m < max(i,j)\leq 2m\\
                  \sum_{r=0}^q\theta_r\theta_{r+|i-j|}, \quad min(i,j)>m,\\
                  0
                \end{array}
              \right.$$
Where applying the innovation algorithim to $W$ yields:
$$
\left\{
                \begin{array}{ll}
                  \hat W_{n+1} = \sum_{j=1}^n\theta_{nj}(W_{n+1-j}-\hat W _{n+1-j}), \quad 1\leq n < m \\
                  \hat W_{n+1} = \sum_{j=1}^q\theta_{nj}(W_{n+1-j}-\hat W_{n+1-j}), \quad n \geq m
                \end{array}
              \right.
$$
This enables us to write $X_t$ as a linear combination of the $W_t$-s vice versa. 

We note:
$$\hat W_{n+1} = P_nW_{n+1}$$
$$\hat X_{n+1} = P_nX_{n+1}$$
Where P is a projection in the hilbert space $L^2(\Omega)$. 

Using this we get 
$$
\left\{
                \begin{array}{ll}
                  \hat W_{t} = \sigma^{-1}\hat X_t, \quad 1 \leq t \leq m \\
                  \hat W_{t} = \sigma^{-1}(\hat X_t - \phi_1X_{t-1}-\dots-\phi_pX_{t-p}),\quad t > m
                \end{array}
              \right.
$$
Combining this we get:
$$X_t - \hat X_t = \sigma(W_t-\hat W_t)$$
which in turns yield:
$$
\hat X_{n+1} = \left\{
                \begin{array}{ll}
                  \sum_{j=1}^n \theta _{nj}(X_{n+1-j}-\hat X_{n+1-j}), \quad 1 \leq t < m \\
                  \phi_1X_n \dots + \phi_pX_{n+1+p}+\sum_{j=1}^q\theta_{nj}X\quad t \geq m
                \end{array}
              \right.
$$

## Model comparison, finding values for p and q
Selecting different values for $p$ and $q$ would yield different models. Using AICC and the AIC criterion is one way of selecting among these. The methods both base themselves on the idea of scoring on the likelihood of seeing the realized values given the model and then to punish the complex models more. We prefer simple models as they are less prone to over-fitting the data. I.e. will mean square error not be able to pick up upon this, so more complex models will always be favored, which is why we should use other criteria as AIC or AICC. 

# Data Analysis
```{r include=FALSE}
#Load data
path = "mlo_data/"
daily_c13iso = read.csv(paste(path, "daily_flask_c13_mlo.csv", sep=""))
monthly_c13iso = read.csv(paste(path, "monthly_flask_c13_mlo.csv", sep=""))
daily_o18iso = read.csv(paste(path, "daily_flask_o18_mlo.csv", sep=""))
intermittent_c13iso = read.csv(paste(path, "intermittent_flask_c14_mlo.csv", sep=""))
monthly_o18iso = read.csv(paste(path, "monthly_flask_o18_mlo.csv", sep=""))
daily_flask = read.csv(paste(path, "daily_flask_CO2_mlo.csv", sep=""))
monthly_flask = read.csv(paste(path, "monthly_flask_CO2_mlo.csv", sep=""), na.strings=-99.99)
ten_min_insitu = read.csv(paste(path, "ten_minute_in_situ_CO2_mlo.txt", sep=""))
daily_insitu = read.csv(paste(path, "daily_in_situ_CO2_mlo.csv", sep=""), comment.char = '%')
weekly_insitu = read.csv(paste(path, "weekly_in_situ_CO2_mlo.csv", sep=""))
monthly_insitu = read.csv(paste(path, "monthly_in_situ_CO2_mlo.csv", sep=""))

ten_min_insitu
#Prep data
#fixes headers
names(monthly_flask) = c( "Yr", "Mn", "Date1", "Date2", "CO2","seasonally", "fit",  "seasonally2", "CO2_filled", "seasonally3")
monthly_flask

#Splitting into two dataframes so that we can delete the NA rows in the non-filled-in data
monthly_flask1 = monthly_flask
monthly_flask1[9:10] = list(NULL) 

monthly_flask2 = monthly_flask
monthly_flask2[5:8] = list(NULL) 
monthly_flask2
na.omit(monthly_flask1)
```
We start by creating simple time series plot to get an overview. 
```{r include=T}
ts = ts(monthly_flask2$CO2_filled[-1],start = c(1960, 2), frequency = 12)
plot(ts, xlab = "Year", ylab = "CO2 level")
title("Observed monthly CO2 levels")

```
As expected we see an increase. There also seems to be a seasonal trend.

We can decompse the timeseries to get a closer look.
```{r include=T}
plot(decompose(ts))
```
With this we see a much clearer sesonal trend.

We plot the autoccorlation plot for the time series 
```{r include=T}
PlotGACF(ts, length(monthly_flask2$CO2_filled), main='Raw-data')
```
Definitely not stationary, apply differencing ($\nabla$) to see if it gets any better. 
```{r include=T}
PlotACF(diff(ts), lag.max = 36,main='Tranformation first difference')
```
Applying $\nabla^2$:
```{r include=T}
PlotACF(diff(diff(ts)), lag.max = 36,main='Tranformation second difference')

```
Does not seem to get any better.

We try capturing the seasonal trend. We expect that a season last for a year, so 12 months seems like a good starting point for differencing. Applying $\nabla^{12}$ we get:
```{r include=T}
# Seems to be some regular trends, after a few months there are tops in the acf. # maybe there is some seasonal variance repeating each 12 months
PlotACF(diff(ts, 12), lag.max = 36,main='Tranformation 12th-difference')
```
With this it seems we have removed the seasonal component. But it is clearly not completly stationary. We try applying $\nabla\nabla^{12}$ to capture the lag: 
```{r include=T}
## Seems better, but still is some acf frome earlier months, following the 
## Sarima way of thinking we can difference some more until it get better
PlotACF(diff(diff(ts, 12),1), lag.max = 36,main='Tranformation 12th-difference')
## The autocorrlation now seems ok, we will not make our model to complex, so we ## decide on using the selected values. 
```
Which seems somewhat stationary, we could increase the lag to get a better fit, but as we prefer simple models, and we want to avoid overfitting, we decide to use $\nabla\nabla^{12}$ to tranform.

We will later see that the acf plot improve form.

There parameters results in a SARIMA model. 
```{r eval=FALSE, include=FALSE}
sarima(ts, p=1, d=1, q=1, P=1, Q=1, D=1, S=12)
```
(Coefficient can i.e. be fit using innovations algoritihm. But that might not be the method impletemented in the package).

To find the best choice of parameters we compare AIC and AICC of different models. 

```{r eval=FALSE, include=FALSE}
## RUNNIING THIS IS QUITE SLOW, WOULD NOT RECCOMEND
minVal.AIC <- c(10000,1,1,1,1)
minVal.AICC <- c(10000,1,1,1,1)
minVal.BIC <- c(10000,1,1,1,1)
errorAt <- c()
for(p1 in 1:5){
  for(q1 in 1:5){
    for(P1 in 1:5){
      for(Q1 in 1:5){
        mod <- tryCatch(sarima(ts, p=p1, d=1, q=q1, P=P1, D=1, S=12, Q = Q1, no.constant = T, tol = 1e-3),
                 error = function(e) "ERROR")
        if(mod == "ERROR"){
          errorAt <- append(errorAt, c(p1, q1, P1, Q1))
          next
        }
        if(mod$AIC < minVal.AIC[1]){
          minVal.AIC <- c(mod$AIC, p1, q1, P1, Q1)
        }
        if(mod$AICc < minVal.AICC[1]){
          minVal.AICC <- c(mod$AICc, p1, q1, P1, Q1)
        }
        if(mod$BIC < minVal.BIC[1]){
          minVal.BIC <- c(mod$BIC, p1, q1, P1, Q1)
        }
        
      }
    }
  }
}
# 4 1 5 1 seems to be the optimal choice of parameters
```
and $p=4, d=1, P=5, Q=1$ seems to be best choice of the remaining parameters, where we get AIC and AICcc values of 1.27 

With this chose of parameters our model would be:
$$\phi(B)\Phi(B)X_t = \theta(B)\Theta(B)Z_ t$$
$$\theta(z)=1+\theta_1z$$
$$\Theta(z)=1+\Theta_1z$$
$$\phi(z)=1-\phi_1z+\dots+\phi_ 4z^4$$
$$\Phi(z)=1-\Phi_1z+\dots+\Phi_5z^5$$


Fitting the model with the selected parameters, we get the following diagnostic:
```{r, echo=FALSE, include = TRUE, message=FALSE}
quiet <- function(x) { 
  sink(tempfile()) 
  on.exit(sink()) 
  invisible(force(x)) 
} 

mod <- quiet(sarima(ts, p=4, d=1, q=1, P=5, Q=1, D=1, S=12))
```
And the following coefficients:
```{r echo=F, include = T}
m <- mod$fit$coef
names(m) <- c("phi1","phi2","phi3","phi4","theta1","Phi1","Phi2","Phi3","Phi4","Phi5","Theta1")
m
```

The standardized residuals seems to be random, and there are not clear trend. The residuals seems homoscedastic and are centered at zero, which is a good indcation that our models assumptions are held by the data.

Looking at the autocorrolation plot, there does not seem to be any lag corrolating that much. Still, there are a few outliers, but that is often on 12+ months and can be considered to be well within the range of some family wise error rate. 

Points in the qq-plot also seems to align with the normal line quite well, which indicate a good fit for our model. 

All in all, the model seem to fit the data quite well. 

## Confidence interval for coefficients
We use bootstrapping to create confidence intervals for the coeffiecient. In using bootstraping we create a simulated data-set by drawing at random from our original data set, and fit a model using this. We repeat this, and in the end get a set of possible coefficient values, which we can use to confidence intervals.


```{r eval=FALSE, include=FALSE}
n.bootstrap <- 200
simulated <- bld.mbb.bootstrap(monthly_flask2$CO2_filled[-1], n.bootstrap)
coefs <- matrix(0, nrow = n.bootstrap, ncol = length(mod$fit$coef))
i <- 1
while(i <= n.bootstrap){
  print(i)
  ts.temp = ts(simulated[[i]],start = c(1960, 2), frequency = 12)
  mod.temp <- tryCatch(sarima(ts.temp, p=4, d=1, q=1, P=5, Q=1, D=1, S=12, tol = 1e-3), error = function(e) NA) # At time optim not too friendly, so select new boot-strap sample. 
  if(is.na(mod.temp)){
    print("ERROR")
    simulated[[i]] = bld.mbb.bootstrap(monthly_flask2$CO2_filled[-1], 2)[[1]]
    next
  }
  print("NO ERROR")
  coefs[i,] <- mod.temp$fit$coef
  i = i + 1
}
write.table(coefs,file="bootstrap_coef.txt") # keeps the rownames
```

For the coefficients we get the following bootstrapped confidence intervals (95%) using 200 samples:
```{r include=FALSE}
coefs2 <- read.table("bootstrap_coef.txt",header=TRUE,row.names=1)
coefs.summary <- apply(coefs2, 2,function(x) quantile(x, c(0.025, 0.975)))
colnames(coefs.summary) <- c("phi1","phi2","phi3","phi4","theta1","Phi1","Phi2","Phi3","Phi4","Phi5","Theta1")
rownames(coefs.summary) <- c("lower 2.5%", "upper 2.5%")
```
```{r echo=FALSE, include=T}
coefs.summary
```




## Forecasting with bootstrapping
Bootstrapping can also be used to obtain prediction interval. The ones gained with traditional methods is often too narrow. [@hyndman_athanasopoulos_2018] We will use a a blocked bootstrap, we select adjunct sections the time series at random and join the selections together.

The method used in this section is adapted from Hyndman and Athanasopoulos's  bootstrapping and bagging chapter. 

```{r include=F, echo=F}
# An example of how bootstrapped series would look like:
ts.bootstrap <- bld.mbb.bootstrap(monthly_flask2$CO2_filled[-1], 100) %>%
  as.data.frame() %>% ts(start=c(1960,2), frequency=12)
```

```{r include=T, eval=F}
autoplot(ts.bootstrap, colour=TRUE) +
  autolayer(ts, colour=FALSE) + guides(colour="none")
  ylab("Bootstrapped series") 
```

We the fit different models, on 200 bootstrap samples:
```{r eval=F}
n.bootstrap <- 200
simulated <- bld.mbb.bootstrap(monthly_flask2$CO2_filled[-1], n.bootstrap)

h <- 360L
future <- matrix(0, nrow=n.bootstrap, ncol=h)
i <- 1
while(i <= n.bootstrap){
  print(i)
  ts.temp = ts(simulated[[i]],start = c(1960, 2), frequency = 12)
  future[i,] <- sarima.for(ts.temp, p=4, d=1, q=1, P=1, Q=1, D=1, S=12, n.ahead = h, plot.all = F)$pred
  
  i = i + 1
}

write.table(future,file="future_bootstrapNew.txt") # keeps the rownames
```

```{r include=FALSE}
future <- read.table("future_bootstrapNew.txt",header=TRUE,row.names=1)

d <- data.frame(monthly_flask2$Yr, monthly_flask2$Mn, monthly_flask2$CO2_filled)
names(d) <- c("Year", "Month", "CO2")

fc.quant <- structure(list(
    mean = ts(colMeans(future), start=c(2019, 11), frequency=12),
    lower = ts(apply(future, 2, quantile, prob=0.025),
               start=c(2019, 11), frequency=12),
    upper = ts(apply(future, 2, quantile, prob=0.975),
               start=c(2019, 11), frequency=12),
    level=95),
  class="forecast")
```
```{r echo=FALSE, include=T, 	fig.height = 4.3}
ts.plot(ts, fc.quant$lower, fc.quant$upper, fc.quant$mean, gpars=list(xlab="Year", ylab="CO2"), col=c("black", "blue", "blue"),xlim=c(1990,2050), main="Modelled flask CO2 levels")

ts.plot(ts, fc.quant$lower, fc.quant$upper, fc.quant$mean, gpars=list(xlab="Year", ylab="CO2"), col=c("black", "blue", "blue"),xlim=c(2030,2045),ylim=c(440,460), main="Modelled flask CO2 levels")

```

```{r eval = F, include = F}
#Inspect the following manually and find first 450
fc.quant$lower 
fc.quant$upper 

```


# Discussion and Conclusion
The model seem to predict an increase of the CO2 levels in the next few years, and the data seems to fit the model assumptions. If we follow the same development as today, and if we assume the model assumption to be true, we will at upper 97.5% in confidence interval reach 450ppm CO2 in 2035, and will at lower 0.025% reach 450ppm in 2038.

# References


C. D. Keeling, S. C. Piper, R. B. Bacastow, M. Wahlen, T. P. Whorf, M. Heimann, and H. A. Meijer, *Exchanges of atmospheric CO2 and 13CO2 with the terrestrial biosphere and oceans from 1978 to 2000.* I. Global aspects, SIO Reference Series , No. 01-06, Scripps Institution of Oceanography, San Diego, 88 pages, 2001.

