---
title: "TMA4285 Timeseries, Exercise 3"
author: "Marius Dioli, Amir Ahmed and Andreas Ferstad"
date: "September 2019"
output:
  pdf_document: default
  word_document: default
  html_document: default
header-includes: null
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	error = FALSE,
	message = FALSE,
	warning = FALSE,
	include = FALSE,
	results = "hold"
)
knitr::opts_chunk$set(error=TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")

```


```{r include=FALSE}
#Imports
library("knitr") #probably already installed
library("rmarkdown") #probably already installed
library(astsa)
library(forecast)
```

# Abstract


# Introduction



# Theory



# Data Analysis
```{r include=FALSE}
#Load data
path = "mlo_data/"
daily_c13iso = read.csv(paste(path, "daily_flask_c13_mlo.csv", sep=""))
monthly_c13iso = read.csv(paste(path, "monthly_flask_c13_mlo.csv", sep=""))
daily_o18iso = read.csv(paste(path, "daily_flask_o18_mlo.csv", sep=""))
intermittent_c13iso = read.csv(paste(path, "intermittent_flask_c14_mlo.csv", sep=""))
monthly_o18iso = read.csv(paste(path, "monthly_flask_o18_mlo.csv", sep=""))
daily_flask = read.csv(paste(path, "daily_flask_co2_mlo.csv", sep=""))
monthly_flask = read.csv(paste(path, "monthly_flask_co2_mlo.csv", sep=""), na.strings=-99.99)
ten_min_insitu = read.csv(paste(path, "ten_minute_in_situ_co2_mlo.txt", sep=""))
daily_insitu = read.csv(paste(path, "daily_in_situ_co2_mlo.csv", sep=""), comment.char = '%')
weekly_insitu = read.csv(paste(path, "weekly_in_situ_co2_mlo.csv", sep=""))
monthly_insitu = read.csv(paste(path, "monthly_in_situ_co2_mlo.csv", sep=""))

ten_min_insitu
#Prep data
#fixes headers
names(monthly_flask) = c( "Yr", "Mn", "Date1", "Date2", "CO2","seasonally", "fit",  "seasonally2", "CO2_filled", "seasonally3")
monthly_flask

#Splitting into two dataframes so that we can delete the NA rows in the non-filled-in data
monthly_flask1 = monthly_flask
monthly_flask1[9:10] = list(NULL) 

monthly_flask2 = monthly_flask
monthly_flask2[5:8] = list(NULL) 
monthly_flask2
na.omit(monthly_flask1)
```
We start by creating simple time series plot to get an overview. 

#Creating basic plots
```{r}
#plot(data.frame(monthly_flask2$Date2, monthly_flask2$CO2_filled), type="o", ylab="CO2 Levels")
#plot.ts(monthly_flask2$CO2_filled, type='s')
```

#Averaging the montly reads so that we have one read per month
```{r}
ts = ts(monthly_flask2$CO2_filled[-1],start = c(1960, 2), frequency = 12)
plot(ts)
plot(decompose(ts))
```
Just decomposing the time series, we see that there is a clear sesonal trend, and a clear increasing general trend. 

We plot the autoccorlation plot for the time series 
```{r}
acf(ts, length(monthly_flask2$CO2_filled), main='Monthly CO2 flask reading')
```
Definitely not stationary, apply differencing, to see if it gets any better. 
```{r}
plot(diff(ts), type='o', main='first difference')
acf(diff(ts), length(monthly_flask2$CO2_filled), main='Monthly CO2 flask reading')

#Nope, we should apply two times differncing

plot(diff(diff(ts)), type='o', main='Second difference')
acf(diff(diff(ts)), length(monthly_flask2$CO2_filled), main='Monthly CO2 flask reading')

# Seems to be some regular trends, after a few months there are tops in the acf. # maybe there is some seasonal variance repeating each 12 months
plot(diff(ts, 12), type='o', main='Second difference')
acf(diff(ts, 12), length(monthly_flask2$CO2_filled), main='Monthly CO2 flask reading')

## Seems better, but still is some acf frome earlier months, following the 
## Sarima way of thinking we can difference some more until it get better
plot(diff(diff(ts, 12),2), type='o', main='Second difference')
acf(diff(diff(ts, 12),1), length(monthly_flask2$CO2_filled), main='Monthly CO2 flask reading')
## The autocorrlation now seems ok, we will not make our model to complex, so we ## decide on using the selected values. 
```

Now plotting the transformed data we get:
```{r}
ts.transformed <- diff(diff(ts, 12),2)
plot(ts.transformed, type = 'o')
```
Which could be a stationary process (we use this although there is some early lag to keep our model less complex and to avoid overfitting).  

So we decide on using  $\nabla\nabla^{12}$ as our tranformation. 

Which results in a SARIMA model. 
```{r}
sarima(ts, p=1, d=1, q=1, P=1, Q=1, D=1, S=12)
```
(Coefficient can i.e. be fit using innovations algoritihm. But that might not be the method impletemented in the package).
To find the best choice of parameters we compare AIC and AICC of different models. 

```{r eval=FALSE, include=FALSE}
## RUNNIING THIS IS QUITE SLOW, WOULD NOT RECCOMEND
minVal.AIC <- c(10000,1,1,1,1)
minVal.AICC <- c(10000,1,1,1,1)
minVal.BIC <- c(10000,1,1,1,1)
errorAt <- c()
for(p1 in 1:5){
  for(q1 in 1:5){
    for(P1 in 1:5){
      for(Q1 in 1:5){
        mod <- tryCatch(sarima(ts, p=p1, d=1, q=q1, P=P1, D=1, S=12, Q = Q1, no.constant = T, tol = 1e-3),
                 error = function(e) "ERROR")
        if(mod == "ERROR"){
          errorAt <- append(errorAt, c(p1, q1, P1, Q1))
          next
        }
        if(mod$AIC < minVal.AIC[1]){
          minVal.AIC <- c(mod$AIC, p1, q1, P1, Q1)
        }
        if(mod$AICc < minVal.AICC[1]){
          minVal.AICC <- c(mod$AICc, p1, q1, P1, Q1)
        }
        if(mod$BIC < minVal.BIC[1]){
          minVal.BIC <- c(mod$BIC, p1, q1, P1, Q1)
        }
        
      }
    }
  }
}
# 4 1 5 1 seems to be the optimal choice of parameters
```
$p=4, d=1, P=5, Q=1$ seems to be best choice of the remaining parameters, where we get AIC and AICcc values of 1.27 

Fitting the model that had the most optimal values:
```{r}
mod <- sarima(ts, p=4, d=1, q=1, P=5, Q=1, D=1, S=12)
mod
```
The model seem to fit the data quite well. 

The standardized residuals seems to be random, and there are not clear trend. The residuals seems homoscedastic and are centered at zero, which is a good indcation that our models assumptions are held by the data.

Looking at the autocorrolation plot, there does not seem to be any lag corrolating that much. Still, there are a few outliers, but that is often on 12+ months and can be considered to be well within the range of some family wise error rate. 

Points in the qq-plot also seems to align with the normal line quite well, which indicate a good fit for our model. 


## Forecasting
We have now fit our model.
```{r}
# Forecasting two years into the future
forecast <- sarima.for(ts, p=4, d=1, q=1, P=1, Q=1, D=1, S=12, n.ahead = 120)
```
The error band in estimation are estimated prediction errors. 

## Bootstrapping to obtain prediction intervals
Bootstrapping can be used to obtain more accurate prediction intervals. The ones gained with traditional methods is often too narrow. In general bootstraping we creare a simulated data-set by drawing at random from our original data set. We repeat this, and fit on our new data, and create prediction interval based on the different observations produced by the different models. We will use a a blocked bootstrap, we select adjunct sections the time series at random and join the part together. 
```{r}
library(forecast)
library(dplyr)
library(ggplot2)
bootseries <- bld.mbb.bootstrap(monthly_flask2$CO2_filled[-1], 100) %>%
  as.data.frame() %>% ts(start=c(1960,2), frequency=12)

autoplot(bootseries, colour=TRUE) +
  autolayer(ts, colour=FALSE) +
  ylab("Bootstrapped series") + guides(colour="none")
```


We the fit different models, on 50 bootstrap samples:
```{r}
nsim <- 10
sim <- bld.mbb.bootstrap(monthly_flask2$CO2_filled[-1], nsim)

h <- 36L
future <- matrix(0, nrow=nsim, ncol=h)
coefs <- matrix(0, nrow = nsim, ncol = length(mod$fit$coef))
for(i in seq(nsim)){
  ts.temp = ts(sim[[i]],start = c(1960, 2), frequency = 12)
  #mod.temp <- sarima(ts.temp, p=4, d=1, q=1, P=5, Q=1, D=1, S=12,details = T,tol = 1e-3)
  #coefs[i,] <- mod.temp$fit$coef
  future[i,] <- sarima.for(ts.temp, p=4, d=1, q=1, P=1, Q=1, D=1, S=12, n.ahead = h, plot.all = F)$pred
}

fc.quant <- structure(list(
    mean = ts(colMeans(future), start=c(2019, 12), frequency=12),
    lower = ts(apply(future, 2, quantile, prob=0.025),
               start=c(2019, 12), frequency=12),
    upper = ts(apply(future, 2, quantile, prob=0.975),
               start=c(2019, 12), frequency=12),
    level=95),
  class="forecast")
library(ggfortify)
autoplot(fc.quant)
autoplot(ts) +
  autolayer(fc.quant, series="Simulated")
```


Property 3.10 Large Sample Distribution of the Estimators
p. 125


# Discussion
Scoring model using information criteria due to the scarcity of data (maybe). 

Talk about uncertainty, known unknowns, and unknown unknowns.

Weaknesses of chosen model and alternative models.

# Conclusion

# Appendix

# References

