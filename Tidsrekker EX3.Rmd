---
title: "TMA4285 Ex3"
author: "Marius Dioli"
date: "September 9, 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	error = FALSE,
	message = FALSE,
	warning = FALSE,
	include = FALSE,
	results = "hold"
)
knitr::opts_chunk$set(error=TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")

```


```{r}
#Imports
library("knitr") #probably already installed
library("rmarkdown") #probably already installed
library(astsa)
```

# Minimum

1. Exploring the data with plotting of relevant statistics
2. Transforming the data
3. Model parameter estimation including uncertainty
4. Diagnostics, and model choice discussion


# Abstract


# Introduction



# Theory


# Data Analysis
```{r}
#Load data
path = "mlo_data/"
marius_path = "C:/Users/mariu/Documents/Time-Series/mlo_data/"
daily_c13iso = read.csv(paste(marius_path, "daily_flask_c13_mlo.csv", sep=""), na.strings=-99.99)
monthly_c13iso = read.csv(paste(marius_path, "monthly_flask_c13_mlo.csv", sep=""), na.strings=-99.99)
daily_o18iso = read.csv(paste(marius_path, "daily_flask_o18_mlo.csv", sep=""), na.strings=-99.99)
intermittent_c13iso = read.csv(paste(marius_path, "intermittent_flask_c14_mlo.csv", sep=""), na.strings=-99.99)
monthly_o18iso = read.csv(paste(marius_path, "monthly_flask_o18_mlo.csv", sep=""), na.strings=-99.99)
daily_flask = read.csv(paste(marius_path, "daily_flask_co2_mlo.csv", sep=""), na.strings=-99.99)
monthly_flask = read.csv(paste(marius_path, "monthly_flask_co2_mlo.csv", sep=""), na.strings=-99.99)
ten_min_insitu = read.csv(paste(marius_path, "ten_minute_in_situ_co2_mlo.txt", sep=""), na.strings=-99.99)
daily_insitu = read.csv(paste(marius_path, "daily_in_situ_co2_mlo.csv", sep=""), comment.char = '%', na.strings=-99.99)
weekly_insitu = read.csv(paste(marius_path, "weekly_in_situ_co2_mlo.csv", sep=""), na.strings=-99.99)
monthly_insitu = read.csv(paste(marius_path, "monthly_in_situ_co2_mlo.csv", sep=""), na.strings=-99.99)


#fixes headers
names(intermittent_c13iso) = c("Date", "C13")
names(monthly_flask) = c( "Yr", "Mn", "Date1", "Date2", "CO2","seasonally", "fit",  "seasonally2", "CO2_filled", "seasonally3")
names(monthly_insitu) = c( "Yr", "Mn", "Date1", "Date2", "CO2","seasonally", "fit",  "seasonally2", "CO2_filled", "seasonally3")
names(monthly_c13iso) = c( "Yr", "Mn", "Date1", "Date2", "C13","seasonally", "fit",  "seasonally2", "C13_filled", "seasonally3")
names(monthly_o18iso) = c( "Yr", "Mn", "Date1", "Date2", "O18","seasonally", "fit",  "seasonally2", "O18_filled", "seasonally3")
names(weekly_insitu) = c("Date", "CO2")
names(daily_flask) = c("Date1", "Date2", "Date3","Date4", "Number_of_flasks", "Flags", "CO2")
names(daily_insitu) = c("Yr", "Mn", "Dy", "CO2", "NB", "Scale")
names(daily_c13iso) = c("Date1", "Date2", "Date3","Date4", "Number_of_flasks", "Flags", "C13")
names(daily_o18iso) = c("Date1", "Date2", "Date3","Date4", "Number_of_flasks", "Flags", "O18")
names(ten_min_insitu) = c('Yr','Mn','Dy','Hr','Mi', 'CO2')




#Splitting into two dataframes so that we can delete the NA rows in the non-filled-in data
monthly_flask1 = monthly_flask
monthly_flask1[9:10] = list(NULL) 

monthly_flask2 = monthly_flask
monthly_flask2[5:8] = list(NULL) 
na.omit(monthly_flask1)
```
We start by creating simple time series plot to get an overview. 
```{r}
#Creating basic plots
plot(data.frame(monthly_flask2$Date2, monthly_flask2$CO2_filled), type="o", ylab="CO2 Levels")
plot(data.frame(monthly_flask1$Date2, monthly_flask1$CO2), type="o", ylab="CO2 Levels")
plot.ts(monthly_flask2$CO2_filled, type='s')

#Converting to timeseries
ts = as.ts(monthly_flask2$CO2_filled, deltat = 1/12)
'?'(ts)
ts = ts[-1]
acf(ts, 100, main='Monthly CO2 flask reading')

#Definitely not stationary, apply differencing. The book states that differencing is a more robust method than decomposition
plot(diff(ts, 12), type='o', main='first difference')
acf2(diff(ts, 12), 100, main='Monthly CO2 flask reading')

#Nope, we should apply two times differncing

plot(diff(diff(ts, 12)), type='o', main='Second difference')
acf2(diff(diff(ts, 12)), main='Monthly CO2 flask reading')


# Seems to be some regular trends, after a few months there are tops in the acf. # maybe there is some seasonal variance repeating each 12 months
plot(diff(ts, 12), type='o', main='Second difference')
acf(diff(ts, 12), length(monthly_flask2$CO2_filled), main='Monthly CO2 flask reading')

## Seems better, but still is some acf frome earlier months, following the 
## Sarima way of thinking we can difference some more until it get better
plot(diff(diff(ts, 12),1), type='o', main='Second difference')
acf(diff(diff(ts, 12),1), main='Monthly CO2 flask reading')
pacf(diff(diff(ts, 12),1), main='Monthly CO2 flask reading')

#Based on the fact that both the acf and pacf function tail off, we conclude that an arma style model fits best, in this case a sarima model.

## The autocorrlation now seems ok, we will not make our model to complex, so we ## decide on using the selected values. 
```
```{r}

#Predictions 
#Actually can be replaced by the sarima.for(timeseries, time, parameters) function, but produces a slightly different plot
pred_plot=function(model_fit, original_timeseries, y_label){
  
fore = predict(mod$fit, n.ahead=300)
forecast = append(original_timeseries, fore$pred)
ts.plot(original_timeseries, xlim=c(0,1000), ylim=c(310, 500), ylab=y_label)
U = fore$pred+fore$se; L = fore$pred-fore$se 
xx = c(time(U), rev(time(U))); yy = c(L, rev(U)) 
polygon(xx, yy, border = 8, col = gray(.6, alpha =.2)) 
lines(fore$pred, col=2)
}

pred_plot(mod$fit, ts, 'Monthly CO2')
```

```{r}
#Monthly in situ

#Plots
ts_monthly_insitu = as.ts(na.omit(monthly_insitu$CO2_filled), deltat=1/12)
plotter(ts_monthly_insitu, 12)
plotter(ts, 12)
#Daily flask and insitu

ts_daily_flask = as.ts(na.omit(daily_flask$CO2), deltat=1/365)
ts_daily_insitu = as.ts(na.omit(daily_insitu$CO2), deltat=1/365)
plot.ts(ts_daily_flask, type='s')
plot.ts(ts_daily_insitu, type='s')

#Insitu lacks outliers
#Differencing and acf
acf2(ts_daily_flask)
acf2(ts_daily_insitu)

#First difference
acf2(diff(ts_daily_flask, 365))
acf2(diff(ts_daily_insitu, 365))


'?'(acf)
#Second difference
acf2(diff(diff(ts_daily_flask, 365)))
acf2(diff(diff(ts_daily_insitu, 365)))


#C13
plotter(as.ts(monthly_c13iso$C13_filled, deltat=1/12), 12)
plotter(as.ts(daily_c13iso$C13, deltat=1/365), 365)

#O18
plotter(as.ts(monthly_o18iso$O18, deltat=1/12), 12)
plotter(as.ts(daily_o18iso$O18, deltat=1/365), 365)
plot(as.ts(monthly_o18iso$O18, deltat=1/12))

#Weekly
plotter(as.ts(weekly_insitu$CO2, deltat=1/52), 52)

#Ten minutes
plotter(as.ts(ten_min_insitu$CO2, deltat=1/525600), 525600)

#Intermittent
plotter(as.ts(intermittent_c13iso$C13, deltat=1/8), 8)



```
We see that the data look broadly the same, but that in situ data tend to have better looking acf functions. Variance seems stable in all our data, so we do not need to transform it. We prefer monthly data over daily data as there are more outliers in the daily data, making it harder to produce a good model.The C13 data and 018 data seem to be mean adjusted in some way, but their acf functions suggested that a two times differencing was appropriate. All the monthly data show broadly similar acf functions after two times differencing, with the only major difference being between flask and in situ data. 


```{r}
#Creating function that makes all the necessary plots of statistics and the function
plotter = function(timeseries, first_diff, name){
  plot.ts(timeseries, type='s')
  acf2(timeseries)
  
  #First differencing
  plot.ts(diff(timeseries, first_diff), type='s')
  acf2(diff(timeseries, first_diff))
  
  #Second differencing
  plot.ts(diff(diff(timeseries, first_diff)), type='s')
  acf2(diff(diff(timeseries, first_diff)))
}
```


Now plotting the transformed data we get:
```{r}
ts.transformed <- diff(diff(ts, 12),2)
plot(ts.transformed, type = 'o')
```
Which seems like a stationary process.  

So we decide on using  $\nabla\nabla^{12}$ as our tranformation. 

Which results in a SARIMA model. As for instance:
```{r}
sarima(ts, p=1, d=2, q=1, P=1, Q=1, D=2, S=12)
#test

```
(Coefficient can i.e. be fit using innovations algoritihm. But that might not be the method impletemented in the package).
To find the best choice of parameters we compare AIC and AICC of different models. 

```{r}
optimal_sarima = function(timeseries){
minVal.AIC <- c(10000,1,1,1,1)
minVal.AICC <- c(10000,1,1,1,1)
minVal.BIC <- c(10000,1,1,1,1)
errorAt <- c()
for(p1 in 4:5){
  for(q1 in 1:5){
    for(P1 in 1:5){
      for(Q1 in 1:5){
        mod <- tryCatch(sarima(ts, p=p1, d=2, q=q1, P=P1, D=2, S=12, Q = Q1, no.constant = T, tol = 1e-3),
                 error = function(e) "ERROR")
        if(mod == "ERROR"){
          errorAt <- append(errorAt, c(p1, q1, P1, Q1))
          next
        }
        if(mod$AIC < minVal.AIC[1]){
          minVal.AIC <- c(mod$AIC, p1, q1, P1, Q1)
        }
        if(mod$AICc < minVal.AICC[1]){
          minVal.AICC <- c(mod$AICc, p1, q1, P1, Q1)
        }
        if(mod$BIC < minVal.BIC[1]){
          minVal.BIC <- c(mod$BIC, p1, q1, P1, Q1)
        }
        
      }
    }
  }
}
return(minVal.AICC)
}
# 4 1 5 1 seems to be best with 1.27 AIC and 1.27 AICC 
```

```{r}
mod <- sarima(ts, p=4, d=2, q=1, P=5, Q=1, D=2, S=12)
mod$ttable
mod

#Predicting 120 months into the future
sarima.for(ts, 120, p=4, d=2, q=1, P=5, Q=1, D=2, S=12)

```

```{r}
#Confidence intervals
#Since we have a large sample size we can use a normal approximation to calculate the confidence intervals to be the estimate plus/minus 
# their estimate/1.96*their standard deviation
ci = c()
for(i in 1:11){
  upper = mod$ttable[i, 1] + 1.96*mod$ttable[i, 2]
  lower = mod$ttable[i, 1] - 1.96*mod$ttable[i, 2]
  ci = rbind(ci, c(mod$ttable[i, 1], lower, upper))
}
ci = as.data.frame(ci)
names(ci) = c("Estimate", "Lower bound", "Upper bound")
ci

#Alternative estimation


```



#Remaining

Plots with the different timeseries' predictions
Discussion
Clean up and prettify things, though probs not really necessary
Save desired plots and give them proper titles


# Discussion
We chose differencing over detrending to remove trend elements in our data as our goal is to coerce the data to stationarity for the purposes of fitting a model (2017 Time Series analysis and Its applications P.56)

We see that models trained on the different kinds of monthly data yield AICC scores and standard errors that are...

Since our data was non-stationary, an ARIMA model seemed appropriate. Our data displays a clear seasonal pattern. The earth's ecosystem in general, and weather in particular, are seasonal, and it is reasonable to assume that co2 levels at a measuring station is correlated with changing weather. Therefore, a model that takes this into account is suitable, leading to our choice of a SARIMA model. Selecting the parameters for our model is done by testing all combinations of parameter values up to 5 and then picking the one with the smallest AICC score. It would seem intuitive to some to set very high values for our paramters to take into account large numbers of past states, but this would lead to an overly complex model and overfitting. It is therefore prudent to explore parameters including only a few previous states in the model.

Drawbacks of this approach are that... Estiamting the error on future states is of course highly uncertain in and of itself.

Our model seems quite robust when inspecting various statistics. The normal QQ plot has only slight deviations at the tails. The standardized residuals appear staionary, and the p values for the Ljung-Box statistic suggest a good fit. The earth's atmosphere is a large enough system that any changes in the trend of CO2 will be gradual, barring some major event. Therefore we would expect accurate predictions a few years into the futre, with decreasing certainty at the 5 years and further out in the future. This seems to be the case in our model.

Alternative models are... One alternative would have been to detrend and deseason the data and then derive an estimate for the noise. This ...

Our models inidicate that CO2 levels will keep rising into the future, increasing by approximately 30ppm 10 years into the future from where the data ends. The rate of increase is approximately linear with some seasonal variation depending on which month of the year we are considering.

# Conclusion

We can conclude from our analysis and model that co2 levels will keep increasing into the future at a rate similar to the current rate.

# Appendix

# References

